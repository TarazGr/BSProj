{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import data_preprocessing as dp\n",
    "import pickle\n",
    "import evaluation as eval\n",
    "\n",
    "DATA_DIR = 'cfp-dataset/Data/Images'\n",
    "TMP_DIR = 'tmp'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "ITERACTIONS = 5000\n",
    "\n",
    "if not os.path.exists(TMP_DIR):\n",
    "    os.makedirs(TMP_DIR)\n",
    "\n",
    "GLOBAL_ITER = dp.global_iteraction(TMP_DIR + '/iteraction.txt')\n",
    "\n",
    "dataset = []\n",
    "category2index = {}\n",
    "index = 0\n",
    "\n",
    "for folder in os.listdir(DATA_DIR):\n",
    "    if folder not in category2index:\n",
    "        category2index[folder] = index\n",
    "        index += 1\n",
    "    imgs = []\n",
    "    directory = os.path.join(DATA_DIR, folder)\n",
    "    for fold in os.listdir(directory):\n",
    "        directory_in = os.path.join(directory, fold)\n",
    "        #print(folder)\n",
    "        for img in os.listdir(directory_in):\n",
    "            img = Image.open(os.path.join(directory_in, img)).convert('L').resize((100, 100))\n",
    "            img = np.asarray(img)\n",
    "            imgs.append(img.reshape(img.shape[0], img.shape[1], 1))\n",
    "        dataset.append(imgs)\n",
    "\n",
    "if not os.path.exists(TMP_DIR + '/train_set.pkl') or not os.path.exists(TMP_DIR + '/test_set.pkl'):\n",
    "    train_set, test_set = dp.split_dataset(dataset, category2index, train_size=0.8)\n",
    "    pickle.dump(train_set, open(TMP_DIR + '/train_set.pkl', 'wb'))\n",
    "    pickle.dump(test_set, open(TMP_DIR + '/test_set.pkl', 'wb'))\n",
    "else:\n",
    "    print('Reloading train set and test set')\n",
    "    train_set = pickle.load(open(TMP_DIR + '/train_set.pkl', 'rb'))\n",
    "    test_set = pickle.load(open(TMP_DIR + '/test_set.pkl', 'rb'))\n",
    "del dataset\n",
    "\n",
    "\n",
    "shape = train_set[0][0].shape\n",
    "#print(shape)\n",
    "\n",
    "### MODEL ###\n",
    "\n",
    "def model(img):\n",
    "\n",
    "    with tf.variable_scope('convolutional_layer_1'):\n",
    "        layer = tf.layers.conv2d(inputs=img, filters=128, kernel_size=[3, 3], padding='same', activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                                 bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01),\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0002))\n",
    "        layer = tf.layers.max_pooling2d(layer, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "    with tf.variable_scope('convolutional_layer_2'):\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=64, kernel_size=[3, 3], padding='same', activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                                 bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01),\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0002))\n",
    "        layer = tf.layers.max_pooling2d(layer, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "    with tf.variable_scope('convolutional_layer_3'):\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=32, kernel_size=[3, 3], padding='same', activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                                 bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01),\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0002))\n",
    "        layer = tf.layers.max_pooling2d(layer, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "    with tf.variable_scope('convolutional_layer_4'):\n",
    "        layer = tf.layers.conv2d(inputs=layer, filters=16, kernel_size=[3, 3], padding='same', activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                                 bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01),\n",
    "                                 kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0002))\n",
    "        layer = tf.layers.max_pooling2d(layer, pool_size=2, strides=2, padding='same')\n",
    "\n",
    "    with tf.variable_scope('flatten_layer'):\n",
    "        layer = tf.contrib.layers.flatten(layer)\n",
    "\n",
    "    with tf.variable_scope('dense_layer_1'):\n",
    "        embeddings = tf.layers.dense(inputs=layer, units=1028, activation=tf.nn.sigmoid,\n",
    "                                     kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01),\n",
    "                                     bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01),\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(1e-3))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    img_1 = tf.placeholder(tf.float32, shape=[None, shape[0], shape[1], shape[2]])\n",
    "    img_2 = tf.placeholder(tf.float32, shape=[None, shape[0], shape[1], shape[2]])\n",
    "    flags = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    with tf.variable_scope('siamese', reuse=tf.AUTO_REUSE) as scope:\n",
    "        embeddings_1 = model(img_1)\n",
    "        embeddings_2 = model(img_2)\n",
    "\n",
    "    distance = tf.abs(embeddings_1 - embeddings_2)\n",
    "\n",
    "    scores = tf.layers.dense(inputs=distance, units=1, activation=tf.nn.sigmoid,\n",
    "                             bias_initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.01))\n",
    "\n",
    "    losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=flags, logits=tf.reshape(scores, shape=[BATCH_SIZE]))\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0005)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.95, use_nesterov=True)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    prediction = tf.cast(tf.argmax(scores, axis=0), dtype=tf.int32)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    writer = tf.summary.FileWriter(TMP_DIR, session.graph)\n",
    "\n",
    "    # reload the model if it exists and continue to train it\n",
    "    try:\n",
    "        saver.restore(session, os.path.join(TMP_DIR, 'model.ckpt'))\n",
    "        print('Model restored')\n",
    "        print('Global epoch:', GLOBAL_ITER)\n",
    "    except:\n",
    "        print('Model initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "\n",
    "    for step in tqdm.tqdm(range(ITERACTIONS), desc='Training Siamese Network'):\n",
    "\n",
    "        batch, label = dp.get_batch(train_set, BATCH_SIZE)\n",
    "\n",
    "        pair_1 = np.array([b[0] for b in batch])\n",
    "        pair_2 = np.array([b[1] for b in batch])\n",
    "\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        _, l = session.run([train_op, loss],\n",
    "                           feed_dict={img_1: pair_1,\n",
    "                                      img_2: pair_2,\n",
    "                                      flags: label},\n",
    "                           run_metadata=run_metadata)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "\n",
    "        # print loss every 500 steps\n",
    "        if (step % 500 == 0 and step > 0) or (step == (ITERACTIONS - 1)):\n",
    "            correct = 0\n",
    "            k = len(test_set) * len(test_set[0])\n",
    "            for _ in range(k):\n",
    "                test, label = dp.get_one_shot_test(test_set)\n",
    "                pair_1 = np.array([b[0] for b in test])\n",
    "                pair_2 = np.array([b[1] for b in test])\n",
    "\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "                pred = session.run(prediction,\n",
    "                                   feed_dict={img_1: pair_1, img_2: pair_2}, run_metadata=run_metadata)\n",
    "                if pred[0] == 0:\n",
    "                    correct += 1\n",
    "\n",
    "            print('Loss:', str(average_loss / step), '\\tAccuracy:', correct / k)\n",
    "        if step == (ITERACTIONS - 1):\n",
    "            writer.add_run_metadata(run_metadata, 'step%d' % step, global_step=GLOBAL_ITER + step + 1)\n",
    "\n",
    "    saver.save(session, os.path.join(TMP_DIR, 'model.ckpt'))\n",
    "    dp.global_iteraction(TMP_DIR + '/iteraction.txt', update=GLOBAL_ITER + step + 1)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
